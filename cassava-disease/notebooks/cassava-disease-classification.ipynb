{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plot\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport fastai\nfrom fastai import vision\n\nimport pretrainedmodels as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the data\n\nTo create the training data set we use standard data augmentation techniques. All parameters defining the transformations used for data augmentation are left at their default values (unless otherwise specified)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"_transform_kwargs = {\"do_flip\": True,\n                     \"flip_vert\": True,  # default is False\n                     \"max_rotate\": 180,  # default is 10\n                     \"max_zoom\": 1.2,    # default is 1.1\n                     \"max_lighting\": 0.2,\n                     \"max_warp\": 0.2,\n                     \"p_affine\": 0.75,\n                     \"p_lighting\": 0.7,\n                    }\n        \n_transforms = vision.get_transforms(**_transform_kwargs)\n\n_data_bunch_kwargs = {\"path\": \"../input/train\",\n                      \"train\": \"train\",\n                      \"valid_pct\": 0.2,\n                      \"bs\": 16,\n                      \"size\": 448,\n                      \"ds_tfms\": _transforms,\n                      \"test\": \"../test/test\"}\n\nimage_data_bunch = (vision.ImageDataBunch\n                          .from_folder(**_data_bunch_kwargs)\n                          .normalize())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_bunch.train_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_bunch.valid_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_bunch.test_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data\n\nAlways important to understand what the images that are being fed into your model actually look like.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_data_bunch.show_batch(figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the model"},{"metadata":{},"cell_type":"markdown","source":"## Transfer Learning\n\nFor computer vision applications always start by trying transfer learning with a standard architecture: [SE-ResNeXt-101](https://arxiv.org/pdf/1803.09820.pdf)."},{"metadata":{"trusted":true},"cell_type":"code","source":"_base_arch = lambda arg: pm.se_resnext101_32x4d(num_classes=1000, pretrained=\"imagenet\")\nlearner = vision.cnn_learner(image_data_bunch,\n                             base_arch=_base_arch,\n                             pretrained=True,\n                             metrics=vision.error_rate,\n                             model_dir=\"/kaggle/working/models/se-resnext101-32x4d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(learner.recorder\n        .plot())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_optimal_lr(recorder):\n    \"\"\"Extract the optimal learning rate from recorder data.\"\"\"\n    optimal_lr = 0\n    minimum_loss = float(\"inf\")\n    for loss, lr in zip(recorder.losses, recorder.lrs):\n        if loss < minimum_loss:\n            optimal_lr = lr\n            minimum_loss = loss\n    return optimal_lr, minimum_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a callback that stores state of \"best\" model.\n# N.B. best model is re-loaded when training completes\n_save_model_kwargs = {\"every\": \"improvement\",\n                      \"monitor\": \"valid_loss\",\n                      \"name\": \"best-model-stage-1\"}\n_save_model = (fastai.callbacks\n                     .SaveModelCallback(learner, **_save_model_kwargs))\n\n# if validation loss < training loss either learning rate too low or not enough training epoch\nlearner.fit_one_cycle(15, callbacks=[_save_model])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the model's predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_interp = (vision.ClassificationInterpretation\n                    .from_learner(learner))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_interp.plot_top_losses(16, figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_interp.plot_top_losses(16, figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_interp.plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_interp.most_confused()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unfreezing, fine-tuning, and learning rates"},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(learner.recorder\n        .plot())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_save_model_kwargs = {\"every\": \"improvement\",\n                      \"monitor\": \"valid_loss\",\n                      \"name\": \"best-model-stage-2\"}\n_save_model = (fastai.callbacks\n                     .SaveModelCallback(learner, **_save_model_kwargs))\nlearner.fit_one_cycle(15, max_lr=slice(1e-6, 1e-4), callbacks=[_save_model])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test-Time Augmentation (TTA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_class_probabilities, _ = learner.TTA(ds_type=fastai.basic_data.DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating a submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"_predicted_classes = (predicted_class_probabilities.argmax(dim=1)\n                                                   .numpy())\n_class_labels = np.array(['cbb','cbsd','cgm','cmd','healthy'])\n_predicted_class_labels = _class_labels[_predicted_classes]\n\n_filenames = np.array([item.name for item in image_data_bunch.test_ds.items])\n\nsubmission = (pd.DataFrame\n                .from_dict({'Category': _predicted_class_labels,'Id': _filenames}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', header=True, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}